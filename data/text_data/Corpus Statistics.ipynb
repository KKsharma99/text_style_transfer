{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.32it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 82.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown, gutenberg, inaugural, stopwords\n",
    "# Number of Sentences, Word Frequency, Mean Sentence Length\n",
    "from tqdm import tqdm\n",
    "import string \n",
    "\n",
    "num_sents_arr = []\n",
    "num_vocab_arr = []\n",
    "mean_size_arr = []\n",
    "avg_word_freq = []\n",
    "tot_word_freq = {}\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "for fileid in tqdm(gutenberg.fileids()):\n",
    "    if fileid == 'bible-kjv.txt' or 'shakespeare' in fileid:\n",
    "        continue\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid))) \n",
    "    \n",
    "    for curr_word in gutenberg.words(fileid):\n",
    "        curr_word = curr_word.lower()\n",
    "        if curr_word not in string.punctuation and curr_word not in stopwords:\n",
    "            if curr_word in tot_word_freq:\n",
    "                tot_word_freq[curr_word] += 1\n",
    "            else:\n",
    "                tot_word_freq[curr_word] = 1\n",
    "    \n",
    "    num_sents_arr.append(num_sents)\n",
    "    mean_size_arr.append(num_words / num_sents)\n",
    "    num_vocab_arr.append(num_vocab)\n",
    "    avg_word_freq.append(num_words / num_vocab)\n",
    "    \n",
    "    # average sentence length, and the number of times each vocabulary item appears in the text on average \n",
    "#     print(round(num_words/num_sents), round(num_words/num_vocab), fileid)\n",
    "\n",
    "for fileid in tqdm(brown.fileids()):\n",
    "    num_words = len(brown.words(fileid))\n",
    "    num_sents = len(brown.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in brown.words(fileid)))\n",
    "    \n",
    "    for curr_word in brown.words(fileid):\n",
    "        curr_word = curr_word.lower()\n",
    "        if curr_word not in string.punctuation and curr_word not in stopwords:\n",
    "            if curr_word in tot_word_freq:\n",
    "                tot_word_freq[curr_word] += 1\n",
    "            else:\n",
    "                tot_word_freq[curr_word] = 1\n",
    "    \n",
    "    num_sents_arr.append(num_sents)\n",
    "    mean_size_arr.append(num_words / num_sents)\n",
    "    num_vocab_arr.append(num_vocab)\n",
    "    avg_word_freq.append(num_words / num_vocab)\n",
    "    \n",
    "    # average sentence length, and the number of times each vocabulary item appears in the text on average \n",
    "#     print(round(num_words/num_sents), round(num_words/num_vocab), fileid) \n",
    "\n",
    "for fileid in tqdm(inaugural.fileids()):\n",
    "    num_words = len(inaugural.words(fileid))\n",
    "    num_sents = len(inaugural.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in inaugural.words(fileid)))\n",
    "    \n",
    "    for curr_word in inaugural.words(fileid):\n",
    "        curr_word = curr_word.lower()\n",
    "        if curr_word not in string.punctuation and curr_word not in stopwords:\n",
    "            if curr_word in tot_word_freq:\n",
    "                tot_word_freq[curr_word] += 1\n",
    "            else:\n",
    "                tot_word_freq[curr_word] = 1\n",
    "    \n",
    "    num_sents_arr.append(num_sents)\n",
    "    mean_size_arr.append(num_words / num_sents)\n",
    "    num_vocab_arr.append(num_vocab)\n",
    "    avg_word_freq.append(num_words / num_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of sentences is 123663.\n",
      "The total number of unique words is 524937.\n",
      "The mean sentence length is 22.924222.\n",
      "The mean number of times each vocab word appears in this corporus is 3.404586.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('--', 9240),\n",
       " ('``', 8837),\n",
       " (\"''\", 8791),\n",
       " ('one', 7686),\n",
       " ('said', 7388),\n",
       " ('would', 6350),\n",
       " ('.\"', 5821),\n",
       " ('could', 5035),\n",
       " (',\"', 4723),\n",
       " ('man', 4050),\n",
       " ('like', 3991),\n",
       " ('time', 3784),\n",
       " ('little', 3666),\n",
       " ('must', 3450),\n",
       " ('well', 3131),\n",
       " ('may', 3124),\n",
       " ('see', 3005),\n",
       " ('much', 2974),\n",
       " ('two', 2953),\n",
       " ('good', 2913),\n",
       " ('first', 2911),\n",
       " ('know', 2904),\n",
       " ('great', 2826),\n",
       " ('upon', 2705),\n",
       " ('never', 2583),\n",
       " ('made', 2561),\n",
       " ('us', 2506),\n",
       " ('new', 2504),\n",
       " ('every', 2446),\n",
       " ('old', 2404)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from heapq import heappush, nlargest\n",
    "\n",
    "'''\n",
    "PROCESS TOTAL ENG CORPUS STATISTICS -- AMONG ALL CORPORA\n",
    "'''\n",
    "\n",
    "tot_sents = sum(num_sents_arr)\n",
    "print('The total number of sentences is %d.' %tot_sents)\n",
    "tot_vocab = sum(num_vocab_arr)\n",
    "print('The total number of unique words is %d.' %tot_vocab)\n",
    "mean_sent_size = sum(mean_size_arr)/len(mean_size_arr)\n",
    "print('The mean sentence length is %f.' %mean_sent_size)\n",
    "mean_vocab_size = sum(avg_word_freq)/len(avg_word_freq)\n",
    "print('The mean number of times each vocab word appears in this corporus is %f.'%mean_vocab_size)\n",
    "\n",
    "word_freq = []\n",
    "for key, value in tot_word_freq.items():\n",
    "    heappush(word_freq, (key, value))\n",
    "nlargest(30, word_freq, key=lambda r:r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Known pre-processing: Lower-case all words for consistency. Filter for punctuation and stop-words.\n",
    "\n",
    "Filter out Bible, Shakespeare pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a69be3cc0add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnum_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnum_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtot_word_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mtot_word_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not bytes"
     ]
    }
   ],
   "source": [
    "# Number of Sentences, Word Frequency, Mean Sentence Length\n",
    "\n",
    "trump_dev = open('/Users/schen1337/Documents/text_style_transfer/data/yelp/dev/negative.txt', 'rb')\n",
    "trump_train = open('/Users/schen1337/Documents/text_style_transfer/data/yelp/train/negative.txt', 'rb')\n",
    "\n",
    "num_words = 0\n",
    "num_sents = 0\n",
    "num_vocab = set()\n",
    "\n",
    "mean_sents_size = []\n",
    "mean_vocab_size = []\n",
    "\n",
    "tot_word_freq = {}\n",
    "\n",
    "for line in trump_dev:\n",
    "    num_sents += 1\n",
    "    for word in line.split(b' '):\n",
    "        num_words += 1\n",
    "        num_vocab.add(word)\n",
    "        if word not in string.punctuation and word not in stopwords:\n",
    "            if word in tot_word_freq:\n",
    "                tot_word_freq[word] += 1\n",
    "            else:\n",
    "                tot_word_freq[word] = 1\n",
    "    mean_sents_size.append(num_words / num_sents)\n",
    "    mean_vocab_size.append(num_words / num_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit5a06139154784f7082b34e4572a2fa43"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
